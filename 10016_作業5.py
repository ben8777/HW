# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aca0Y_5CaFRcomgodLHx4UpE87ecf7Gm
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import matplotlib.pyplot as plt
import os
import requests
import zipfile
import io
import pickle
import random
from typing import Union, Tuple, Dict, Any
from sklearn.preprocessing import LabelEncoder

def seed_everything(seed=42):
    """Set seeds for reproducibility."""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

def fetch_data(url: str, save_dir: str, filename: str) -> pd.DataFrame:
    """Download and extract data from a zip file URL."""
    # Create directory if it doesn't exist
    os.makedirs(save_dir, exist_ok=True)

    req = requests.get(url)
    zip_file = zipfile.ZipFile(io.BytesIO(req.content))
    zip_file.extractall(save_dir)

    df = pd.read_csv(os.path.join(save_dir, filename), sep='\t', header=None)
    return df

def shape_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Split and prepare the dataset for training/validation/testing."""
    df.columns = ['id', 'title', 'url', 'publisher', 'category', 'story', 'hostname', 'timestamp']
    # Filter for specific publishers
    publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']
    df_filtered = df[df['publisher'].isin(publishers)]

    # Split data
    df_train, df_temp = train_test_split(df_filtered, test_size=0.2, random_state=42)
    df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)

    return df_train, df_val, df_test

def fetch_shape_save_data(url: str, save_dir: str, filename: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Fetch, shape, and save the data to disk."""
    df = fetch_data(url=url, save_dir=save_dir, filename=filename)
    train_df, valid_df, test_df = shape_data(df=df)

    # Save the datasets
    train_df.to_csv(os.path.join(save_dir, 'train.txt'), sep='\t', index=None)
    valid_df.to_csv(os.path.join(save_dir, 'val.txt'), sep='\t', index=None)
    test_df.to_csv(os.path.join(save_dir, 'test.txt'), sep='\t', index=None)

    print('Training set size:', len(train_df))
    print('Validation set size:', len(valid_df))
    print('Test set size:', len(test_df))

    return train_df, valid_df, test_df

class NewsClassifier:
    """End-to-end news classification pipeline."""

    def __init__(self, vectorizer_params=None, model_type='LR', model_params=None):
        self.model_type = model_type
        self.vectorizer_params = vectorizer_params or {}
        self.model_params = model_params or {}
        self.pipeline = self._create_pipeline()
        self.classes_ = None
        self.label_encoder = None

    def _create_pipeline(self):
        """Create a scikit-learn pipeline with vectorizer and classifier."""
        vectorizer = TfidfVectorizer(**self.vectorizer_params)

        if self.model_type == 'LR':
            classifier = LogisticRegression(**self.model_params)
        elif self.model_type == 'NB':
            classifier = MultinomialNB(**self.model_params)
        elif self.model_type == 'RF':
            classifier = RandomForestClassifier(**self.model_params)
        elif self.model_type == 'XGB':
            classifier = xgb.XGBClassifier(**self.model_params)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

        return Pipeline([
            ('vectorizer', vectorizer),
            ('classifier', classifier)
        ])

    def fit(self, X, y):
        """Train the model on the provided data."""
        # Encode target variable for XGBoost if necessary
        if self.model_type == 'XGB':
            self.label_encoder = LabelEncoder()
            y_encoded = self.label_encoder.fit_transform(y)
            self.classes_ = self.label_encoder.classes_
        else:
            y_encoded = y
            self.classes_ = np.unique(y)

        self.pipeline.fit(X, y_encoded)
        return self

    def predict(self, X):
        """Make predictions on new data."""
        predictions = self.pipeline.predict(X)
        # Decode predictions back to original class names if LabelEncoder was used
        if self.model_type == 'XGB' and self.label_encoder:
            return self.label_encoder.inverse_transform(predictions)
        return predictions

    def predict_proba(self, X):
        """Get prediction probabilities for each class."""
        return self.pipeline.predict_proba(X)

    def save(self, filepath):
        """Save the trained model to disk."""
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, filepath):
        """Load a trained model from disk."""
        with open(filepath, 'rb') as f:
            return pickle.load(f)

    def get_feature_importance(self, top_n=10):
        """Extract feature importance information."""
        vectorizer = self.pipeline.named_steps['vectorizer']
        classifier = self.pipeline.named_steps['classifier']

        feature_names = vectorizer.get_feature_names_out()

        if self.model_type in ('LR', 'NB'):
            # For LogisticRegression and Naive Bayes
            importance_per_class = []

            if self.model_type == 'LR':
                coefficients = classifier.coef_
            else:  # NB
                coefficients = classifier.feature_log_prob_

            # Use stored original class names
            class_names = self.classes_

            for i, coefs in enumerate(coefficients):
                class_name = class_names[i]
                top_indices = coefs.argsort()[-top_n:][::-1]

                top_features = [(feature_names[idx], coefs[idx]) for idx in top_indices]
                importance_per_class.append((class_name, top_features))

            return importance_per_class

        elif self.model_type == 'RF':
            # For Random Forest
            importances = classifier.feature_importances_
            indices = importances.argsort()[-top_n:][::-1]
            return [(feature_names[idx], importances[idx]) for idx in indices]

        elif self.model_type == 'XGB':
            # For XGBoost
            importances = classifier.feature_importances_
            indices = importances.argsort()[-top_n:][::-1]
            return [(feature_names[idx], importances[idx]) for idx in indices]

        return None

def evaluate_model(classifier, X_train, y_train, X_val, y_val, X_test, y_test):
    """Evaluate model performance on train/val/test sets."""
    results = {}

    for name, X, y in [('train', X_train, y_train),
                      ('validation', X_val, y_val),
                      ('test', X_test, y_test)]:

        y_pred = classifier.predict(X)
        accuracy = accuracy_score(y, y_pred)
        report = classification_report(y, y_pred, output_dict=True)
        conf_matrix = confusion_matrix(y, y_pred)

        # Calculate micro and macro averages manually
        precision_micro = precision_score(y, y_pred, average='micro')
        recall_micro = recall_score(y, y_pred, average='micro')
        f1_micro = f1_score(y, y_pred, average='micro')

        precision_macro = precision_score(y, y_pred, average='macro')
        recall_macro = recall_score(y, y_pred, average='macro')
        f1_macro = f1_score(y, y_pred, average='macro')

        results[name] = {
            'accuracy': accuracy,
            'report': report,
            'confusion_matrix': conf_matrix,
            'micro_avg': {
                'precision': precision_micro,
                'recall': recall_micro,
                'f1': f1_micro
            },
            'macro_avg': {
                'precision': precision_macro,
                'recall': recall_macro,
                'f1': f1_macro
            }
        }

    return results

def plot_confusion_matrix(classifier, X_test, y_test, class_names, filename='confusion_matrix.png'):
    """Plot and save confusion matrix."""
    from sklearn.metrics import ConfusionMatrixDisplay

    y_pred = classifier.predict(X_test)

    # Calculate and print the confusion matrix for a detailed view
    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix:")
    print(cm)

    # Print the normalized confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("\nNormalized Confusion Matrix:")
    print(np.round(cm_normalized, 2))

    fig, ax = plt.subplots(figsize=(10, 8))
    # Pass original class names to ConfusionMatrixDisplay
    disp = ConfusionMatrixDisplay.from_predictions(
        y_test, y_pred, display_labels=class_names, ax=ax
    )
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()

def compute_detailed_metrics(y_true, y_pred, class_names):
    """Compute detailed precision, recall, and F1 score for each class."""
    # Calculate detailed metrics
    detailed_metrics = classification_report(y_true, y_pred, output_dict=True)

    # Print metrics for each class
    print("\nDetailed Metrics by Category:")
    print("Category\tPrecision\tRecall\t\tF1-Score")
    print("-" * 60)

    for class_name in class_names:
        if class_name in detailed_metrics:
            precision = detailed_metrics[class_name]['precision']
            recall = detailed_metrics[class_name]['recall']
            f1 = detailed_metrics[class_name]['f1-score']
            print(f"{class_name}\t\t{precision:.4f}\t\t{recall:.4f}\t\t{f1:.4f}")

    # Calculate and print micro and macro averages
    micro_precision = precision_score(y_true, y_pred, average='micro')
    micro_recall = recall_score(y_true, y_pred, average='micro')
    micro_f1 = f1_score(y_true, y_pred, average='micro')

    macro_precision = precision_score(y_true, y_pred, average='macro')
    macro_recall = recall_score(y_true, y_pred, average='macro')
    macro_f1 = f1_score(y_true, y_pred, average='macro')

    print("-" * 60)
    print(f"Micro Avg\t{micro_precision:.4f}\t\t{micro_recall:.4f}\t\t{micro_f1:.4f}")
    print(f"Macro Avg\t{macro_precision:.4f}\t\t{macro_recall:.4f}\t\t{macro_f1:.4f}")

def plot_regularization_curves(X_train, y_train, X_val, y_val, X_test, y_test, model_type='LR', filename='reg_curve.png'):
    """Plot how regularization affects model performance."""

    # For logistic regression, we're looking at the C parameter (inverse of regularization strength)
    if model_type == 'LR':
        param_name = 'C'
        param_values = [0.001, 0.01, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0]
    elif model_type == 'NB':
        param_name = 'alpha'
        param_values = [0.001, 0.01, 0.1, 0.3, 1.0, 3.0, 10.0]
    elif model_type == 'RF':
        param_name = 'n_estimators'
        param_values = [10, 50, 100, 200, 300, 500]
    elif model_type == 'XGB':
        param_name = 'reg_lambda'  # L2 regularization
        param_values = [0.001, 0.01, 0.1, 0.3, 1.0, 3.0, 10.0]
    else:
        raise ValueError(f"Unsupported model type for regularization curve: {model_type}")

    train_scores = []
    val_scores = []
    test_scores = []

    # Create results table for capturing all metrics
    results_table = []

    for param_value in param_values:
        # Create model with current parameter value
        model_params = {param_name: param_value}
        classifier = NewsClassifier(model_type=model_type, model_params=model_params)

        # Train model
        classifier.fit(X_train, y_train)

        # Calculate scores
        train_pred = classifier.predict(X_train)
        val_pred = classifier.predict(X_val)
        test_pred = classifier.predict(X_test)

        train_score = accuracy_score(y_train, train_pred)
        val_score = accuracy_score(y_val, val_pred)
        test_score = accuracy_score(y_test, test_pred)

        train_scores.append(train_score)
        val_scores.append(val_score)
        test_scores.append(test_score)

        # Store detailed metrics for this parameter value
        results_table.append({
            param_name: param_value,
            'train_accuracy': train_score,
            'val_accuracy': val_score,
            'test_accuracy': test_score,
            'train_precision_micro': precision_score(y_train, train_pred, average='micro'),
            'train_recall_micro': recall_score(y_train, train_pred, average='micro'),
            'train_f1_micro': f1_score(y_train, train_pred, average='micro'),
            'val_precision_micro': precision_score(y_val, val_pred, average='micro'),
            'val_recall_micro': recall_score(y_val, val_pred, average='micro'),
            'val_f1_micro': f1_score(y_val, val_pred, average='micro'),
            'test_precision_micro': precision_score(y_test, test_pred, average='micro'),
            'test_recall_micro': recall_score(y_test, test_pred, average='micro'),
            'test_f1_micro': f1_score(y_test, test_pred, average='micro')
        })

    # Convert results to DataFrame and print
    results_df = pd.DataFrame(results_table)
    print(f"\nRegularization Parameter Study for {model_type} Model:")
    print(results_df.to_string(index=False))

    # Determine best parameter value based on validation accuracy
    best_val_idx = np.argmax(val_scores)
    best_param = param_values[best_val_idx]
    print(f"\nBest {param_name} value based on validation accuracy: {best_param}")
    print(f"With validation accuracy: {val_scores[best_val_idx]:.4f}")
    print(f"And test accuracy: {test_scores[best_val_idx]:.4f}")

    # Plot results
    plt.figure(figsize=(10, 6))
    plt.grid()
    plt.semilogx(param_values, train_scores, 'o-', color='blue', label='Training')
    plt.semilogx(param_values, val_scores, 'o-', color='green', label='Validation')
    plt.semilogx(param_values, test_scores, 'o-', color='red', label='Test')
    plt.title(f'Regularization Curve for {model_type}')
    plt.xlabel(f'Regularization parameter ({param_name})')
    plt.ylabel('Accuracy')
    plt.legend(loc='best')
    plt.savefig(filename)
    plt.close()

    return results_df, best_param

def main():
    """Run the full news classification pipeline with detailed evaluation for logistic regression."""
    seed_everything(42)

    # Data paths and directories
    save_dir = './data/'
    url = "https://archive.ics.uci.edu/static/public/359/news+aggregator.zip"
    filename = 'newsCorpora.csv'

    print("\nFetching and preparing data...")
    df_train, df_val, df_test = fetch_shape_save_data(url=url, save_dir=save_dir, filename=filename)

    # Prepare data
    X_train = df_train['title']
    y_train = df_train['category']
    X_val = df_val['title']
    y_val = df_val['category']
    X_test = df_test['title']
    y_test = df_test['category']

    print("\nTraining Logistic Regression model...")
    # Train logistic regression model
    lr_params = {'C': 1.0, 'max_iter': 1000}
    lr_classifier = NewsClassifier(model_type='LR', model_params=lr_params)
    lr_classifier.fit(X_train, y_train)

    # Save the model
    model_path = os.path.join(save_dir, 'model_lr.pkl')
    lr_classifier.save(model_path)

    # Get class names
    class_names = lr_classifier.classes_

    # Evaluate model
    print("\nEvaluating Logistic Regression model...")
    lr_evaluation = evaluate_model(lr_classifier, X_train, y_train, X_val, y_val, X_test, y_test)

    # Print test results
    print("\nLogistic Regression test accuracy:", lr_evaluation['test']['accuracy'])

    # Plot and print detailed confusion matrix
    print("\nGenerating confusion matrix...")
    plot_confusion_matrix(
        lr_classifier, X_test, y_test, class_names,
        filename=os.path.join(save_dir, 'confusion_matrix_lr.png')
    )

    # Compute detailed metrics for Logistic Regression
    print("\nComputing detailed metrics for Logistic Regression...")
    y_pred_test = lr_classifier.predict(X_test)
    compute_detailed_metrics(y_test, y_pred_test, class_names)

    # Run regularization curve analysis for Logistic Regression
    print("\n")
    results_df, best_param = plot_regularization_curves(
        X_train, y_train, X_val, y_val, X_test, y_test,
        model_type='LR',
        filename=os.path.join(save_dir, 'regularization_curve_lr.png')
    )

    # Train a model with the best parameter
    print(f"\nTraining final Logistic Regression model with C={best_param}...")
    best_lr_classifier = NewsClassifier(model_type='LR', model_params={'C': best_param, 'max_iter': 1000})
    best_lr_classifier.fit(X_train, y_train)

    # Evaluate the best model
    best_lr_evaluation = evaluate_model(best_lr_classifier, X_train, y_train, X_val, y_val, X_test, y_test)

    print("\nFinal Logistic Regression model performance:")
    print(f"Test accuracy: {best_lr_evaluation['test']['accuracy']:.4f}")
    print(f"Test precision (micro): {best_lr_evaluation['test']['micro_avg']['precision']:.4f}")
    print(f"Test recall (micro): {best_lr_evaluation['test']['micro_avg']['recall']:.4f}")
    print(f"Test F1 (micro): {best_lr_evaluation['test']['micro_avg']['f1']:.4f}")
    print(f"Test precision (macro): {best_lr_evaluation['test']['macro_avg']['precision']:.4f}")
    print(f"Test recall (macro): {best_lr_evaluation['test']['macro_avg']['recall']:.4f}")
    print(f"Test F1 (macro): {best_lr_evaluation['test']['macro_avg']['f1']:.4f}")

    # Save the best model
    best_model_path = os.path.join(save_dir, 'best_model_lr.pkl')
    best_lr_classifier.save(best_model_path)

    print("\nNews classification pipeline completed successfully!")

if __name__ == "__main__":
    main()