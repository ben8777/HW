# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X7L4FV05_o0rxI9PrQnrsCs6iDveep3W
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import re
from collections import Counter
import matplotlib.pyplot as plt

def read_conll(filepath):
    sentences = []
    sentence = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split('\t')
                word = {
                    'text': parts[1],
                    'lemma': parts[2],
                    'pos': parts[3]
                }
                sentence.append(word)
            elif sentence:
                sentences.append(sentence)
                sentence = []
        if sentence:
            sentences.append(sentence)
    return sentences

def extract_verbs_surface(sentences):
    verbs = []
    for sentence in sentences:
        for word in sentence:
            if word['pos'].startswith('V') and re.match(r'\w+', word['text']):
                verbs.append(word['text'])
    return verbs

def extract_verbs_lemma(sentences):
    verbs = []
    for sentence in sentences:
        for word in sentence:
            if word['pos'].startswith('V') and re.match(r'\w+', word['text']):
                verbs.append(word['lemma'])
    return verbs

def extract_a_of_b(sentences):
    phrases = []
    for sentence in sentences:
        for i in range(len(sentence) - 2):
            if sentence[i]['pos'].startswith('N') and sentence[i + 1]['text'].lower() == 'of' and sentence[i + 2]['pos'].startswith('N') and re.match(r'\w+', sentence[i]['text']) and re.match(r'\w+', sentence[i+2]['text']):
                phrases.append((sentence[i]['text'], sentence[i + 2]['text']))
    return phrases

def longest_noun_sequence(sentences):
    longest_seq = []
    current_seq = []
    for sentence in sentences:
        for word in sentence:
            if word['pos'].startswith('N') and re.match(r'\w+', word['text']):
                current_seq.append(word['text'])
            else:
                if len(current_seq) > len(longest_seq):
                    longest_seq = current_seq
                current_seq = []
        if len(current_seq) > len(longest_seq):
            longest_seq = current_seq
        current_seq = []
    return longest_seq

def word_frequency(sentences):
    words = []
    for sentence in sentences:
        for word in sentence:
            if re.match(r'\w+', word['text']):
                words.append(word['text'].lower())
    return Counter(words).most_common()

def top_10_words(frequencies):
    words, counts = zip(*frequencies[:10])
    plt.figure(figsize=(10, 5))
    plt.bar(words, counts)
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title('Top 10 Most Frequent Words')
    plt.show()

def co_occurring_with_alice(sentences):
    alice_words = []
    for sentence in sentences:
        for i, word in enumerate(sentence):
            if word['text'].lower() == 'alice':
                for j in range(max(0, i - 5), min(len(sentence), i + 6)):
                    if j != i and re.match(r'\w+', sentence[j]['text']):
                        alice_words.append(sentence[j]['text'].lower())
    return Counter(alice_words).most_common()

def histogram_word_frequencies(frequencies):
    counts = [count for _, count in frequencies]
    freq_counts = Counter(counts)
    freqs, num_words = zip(*sorted(freq_counts.items()))
    plt.figure(figsize=(10, 5))
    plt.bar(freqs, num_words)
    plt.xlabel('Word Frequency')
    plt.ylabel('Number of Unique Words')
    plt.title('Histogram of Word Frequencies')
    plt.show()

def zipfs_law(frequencies):
    ranks = range(1, len(frequencies) + 1)
    counts = [count for _, count in frequencies]
    plt.figure(figsize=(10, 5))
    plt.loglog(ranks, counts)
    plt.xlabel('Rank')
    plt.ylabel('Frequency')
    plt.title("Zipf's Law")
    plt.show()

# Main execution
zip_filepath = 'alice.zip'
conll_filepath = 'alice.txt.conll'

with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:
    zip_ref.extract(conll_filepath)

sentences = read_conll(conll_filepath)

# 31. Verbs
verbs_surface = extract_verbs_surface(sentences)
print("31. Verbs (Surface Form):", verbs_surface[:10])

# 32. Verbs
verbs_lemma = extract_verbs_lemma(sentences)
print("32. Verbs (Lemmas):", verbs_lemma[:10])

# 33. A of B
a_of_b_phrases = extract_a_of_b(sentences)
print("33. 'A of B' Phrases:", a_of_b_phrases[:10])

# 34. Longest Noun Sequence
longest_noun_seq = longest_noun_sequence(sentences)
print("34. Longest Noun Sequence:", longest_noun_seq)

# 35. Word Frequency
word_freq = word_frequency(sentences)
print("35. Word Frequency:", word_freq[:10])

# 36. Top 10 Most Frequent Words
top_10_words(word_freq)

# 37. Top 10 Words with "Alice"
alice_cooc = co_occurring_with_alice(sentences)
top_10_words(alice_cooc)

# 38. Histogram of Word Frequencies
histogram_word_frequencies(word_freq)

# 39. Zipf's Law
zipfs_law(word_freq)